# ZEA Sport Platform - IntegraciÃ³n con Cerebelum

GuÃ­a completa de integraciÃ³n de Cerebelum Workflows con tu aplicaciÃ³n FastAPI existente.

---

## ğŸ“‹ Tabla de Contenidos

1. [Arquitectura](#arquitectura)
2. [Setup Inicial](#setup-inicial)
3. [IntegraciÃ³n con FastAPI](#integraciÃ³n-con-fastapi)
4. [Workflows Implementados](#workflows-implementados)
5. [Acceso a tu Base de Datos](#acceso-a-tu-base-de-datos)
6. [Docker Compose](#docker-compose)
7. [Desarrollo Local](#desarrollo-local)
8. [Monitoreo y Debugging](#monitoreo-y-debugging)
9. [Preguntas Frecuentes](#preguntas-frecuentes)

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Tu AplicaciÃ³n FastAPI (Puerto 8000)         â”‚
â”‚                                                             â”‚
â”‚  - REST endpoints                                           â”‚
â”‚  - Clean Architecture (Use Cases + Repositories)            â”‚
â”‚  - PostgreSQL (tu DB existente)                            â”‚
â”‚                                                             â”‚
â”‚  from cerebelum import DistributedExecutor                  â”‚
â”‚  executor = DistributedExecutor("localhost:9090")           â”‚
â”‚  await executor.execute(workflow, inputs)                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ gRPC (Python SDK)
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Cerebelum Core (Puerto 9090)                       â”‚
â”‚                                                             â”‚
â”‚  - Workflow Orchestration (Elixir/OTP)                      â”‚
â”‚  - Event Store (PostgreSQL separado)                        â”‚
â”‚  - State Management + Resurrection                          â”‚
â”‚  - gRPC Server                                              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ gRPC (Task Assignment)
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Python Workers (Tu LÃ³gica de Negocio)          â”‚
â”‚                                                             â”‚
â”‚  from cerebelum import Worker                               â”‚
â”‚  worker = Worker(core_url="localhost:9090")                 â”‚
â”‚  await worker.run()                                         â”‚
â”‚                                                             â”‚
â”‚  - Ejecutan steps de workflows                              â”‚
â”‚  - Importan tus repositorios existentes                     â”‚
â”‚  - Acceden a tu PostgreSQL                                  â”‚
â”‚  - Usan tus casos de uso                                    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Puntos Clave:**
- âœ… **FastAPI sigue siendo tu aplicaciÃ³n principal** (REST API, UI, etc.)
- âœ… **Cerebelum Core corre como servicio separado** (orquestaciÃ³n de workflows)
- âœ… **Workers son procesos Python** que importan tu cÃ³digo existente
- âœ… **2 bases de datos PostgreSQL separadas**:
  - Tu DB existente (usuarios, bookings, etc.)
  - Cerebelum DB (solo event sourcing, no afecta tu schema)

---

## ğŸš€ Setup Inicial

### 1. Instalar Dependencias

```bash
# En tu proyecto FastAPI
pip install cerebelum-sdk  # TODO: publicar en PyPI

# O desde el repo:
cd /path/to/cerebelum-core/examples/python-sdk
pip install -e .
```

### 2. ConfiguraciÃ³n de Entorno

Crear `.env` en tu proyecto FastAPI:

```bash
# Tu DB existente (sin cambios)
DATABASE_URL=postgresql://user:pass@localhost:5432/zea_sport_db

# Cerebelum Core
CEREBELUM_CORE_URL=localhost:9090

# Workers
CEREBELUM_WORKER_ID=zea-sport-worker-${HOSTNAME:-local}
```

### 3. Estructura de Archivos Recomendada

```
your-fastapi-app/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ endpoints/
â”‚   â”‚       â”œâ”€â”€ auth.py
â”‚   â”‚       â”œâ”€â”€ bookings.py
â”‚   â”‚       â””â”€â”€ admin.py
â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”œâ”€â”€ use_cases/
â”‚   â”‚   â””â”€â”€ repositories/
â”‚   â”œâ”€â”€ workflows/              # â† NUEVO
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ athlete_onboarding.py
â”‚   â”‚   â”œâ”€â”€ booking_request.py
â”‚   â”‚   â”œâ”€â”€ session_completion.py
â”‚   â”‚   â””â”€â”€ payment_report.py
â”‚   â””â”€â”€ workers/                # â† NUEVO
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ main.py             # Worker principal
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â””â”€â”€ requirements.txt
```

---

## ğŸ”Œ IntegraciÃ³n con FastAPI

### Paso 1: Definir Workflows

En `app/workflows/athlete_onboarding.py`:

```python
from cerebelum import WorkflowBuilder
from app.domain.repositories.user_repository import UserRepository
from app.domain.repositories.athlete_repository import AthleteRepository

# Importar tus repositorios existentes
user_repo = UserRepository()
athlete_repo = AthleteRepository()


async def validate_registration(ctx, inputs):
    """Step que usa tu repository existente."""
    user_id = inputs['user_id']

    # Usar tu cÃ³digo existente
    user = await user_repo.get_by_id(user_id)

    if not user:
        raise Exception(f"User {user_id} not found")

    return {
        'user_id': user.id,
        'email': user.email,
        'registration_valid': True
    }


async def request_profile_completion(ctx, inputs):
    """Espera hasta 7 dÃ­as que el atleta complete el perfil."""
    user_id = inputs['user_id']

    approval_result = await ctx.request_approval(
        approval_type="athlete_profile_completion",
        approval_data={"user_id": user_id},
        timeout_ms=7 * 24 * 60 * 60 * 1000  # 7 dÃ­as
    )

    if approval_result.approved:
        return approval_result.data
    else:
        raise Exception("Profile not completed in time")


async def enable_booking_capability(ctx, inputs):
    """Habilita al atleta para hacer bookings."""
    user_id = ctx.execution_id.split('-')[0]

    # Usar tu repository
    await athlete_repo.update(user_id, {
        'can_book': True,
        'profile_completed_at': datetime.utcnow()
    })

    return {'can_book': True}


def build_athlete_onboarding_workflow():
    return (
        WorkflowBuilder("zea_sport.AthleteOnboarding")
        .timeline([
            "validate_registration",
            "request_profile_completion",
            "enable_booking_capability"
        ])
        .step("validate_registration", validate_registration)
        .step("request_profile_completion", request_profile_completion)
        .step("enable_booking_capability", enable_booking_capability)
        .build()
    )
```

### Paso 2: Ejecutar desde Endpoints

En `app/api/endpoints/auth.py`:

```python
from fastapi import APIRouter, Depends
from cerebelum import DistributedExecutor
from app.workflows.athlete_onboarding import build_athlete_onboarding_workflow
from app.domain.use_cases.register_user import RegisterUserUseCase
import os

router = APIRouter()


@router.post("/register")
async def register_user(data: RegisterData):
    """
    Endpoint de registro con workflow de onboarding.
    """
    # 1. Crear usuario usando tu caso de uso existente
    use_case = RegisterUserUseCase()
    user = await use_case.execute(data.email, data.password)

    # 2. Ejecutar workflow de onboarding
    executor = DistributedExecutor(
        core_url=os.getenv('CEREBELUM_CORE_URL', 'localhost:9090')
    )

    result = await executor.execute(
        build_athlete_onboarding_workflow(),
        {
            'user_id': user.id,
            'email': user.email
        }
    )

    # 3. Retornar respuesta inmediata
    return {
        'user_id': user.id,
        'onboarding_execution_id': result.execution_id,
        'message': 'Registration successful. Please complete your profile.'
    }
```

En `app/api/endpoints/bookings.py`:

```python
from fastapi import APIRouter
from cerebelum import DistributedExecutor
from app.workflows.booking_request import build_booking_request_workflow

router = APIRouter()


@router.post("/bookings")
async def create_booking(data: BookingRequest, current_user: User = Depends(get_current_user)):
    """
    Endpoint para crear una reserva.
    """
    executor = DistributedExecutor(core_url=os.getenv('CEREBELUM_CORE_URL'))

    result = await executor.execute(
        build_booking_request_workflow(),
        {
            'athlete_id': current_user.id,
            'coach_id': data.coach_id,
            'slot_datetime': data.slot_datetime.isoformat(),
            'athlete_notes': data.notes
        }
    )

    # Workflow es rÃ¡pido (<1s), retorna booking confirmado
    return {
        'booking_id': result.output.get('booking_id'),
        'status': 'CONFIRMED',
        'execution_id': result.execution_id
    }


@router.post("/bookings/{booking_id}/complete")
async def complete_session(
    booking_id: str,
    data: CompleteSessionData,
    current_user: User = Depends(get_current_user)
):
    """
    Endpoint para que el coach marque la sesiÃ³n como completada.
    """
    from app.workflows.session_completion import build_session_completion_workflow

    executor = DistributedExecutor(core_url=os.getenv('CEREBELUM_CORE_URL'))

    result = await executor.execute(
        build_session_completion_workflow(),
        {
            'booking_id': booking_id,
            'athlete_id': data.athlete_id,
            'coach_id': current_user.id,
            'actual_start_time': data.actual_start_time.isoformat(),
            'actual_end_time': data.actual_end_time.isoformat()
        }
    )

    # Workflow queda activo esperando evaluaciones (dÃ­as)
    return {
        'message': 'Session completed. Feedback requests sent.',
        'execution_id': result.execution_id,
        'billable_minutes': result.output.get('actual_duration_minutes')
    }
```

### Paso 3: Endpoint para Aprobar Workflows

En `app/api/endpoints/feedback.py`:

```python
from fastapi import APIRouter
from cerebelum import DistributedExecutor

router = APIRouter()


@router.post("/feedback/submit")
async def submit_feedback(data: FeedbackData, current_user: User = Depends(get_current_user)):
    """
    Endpoint para que atleta/coach envÃ­en feedback.
    """
    # Aprobar el workflow correspondiente
    executor = DistributedExecutor(core_url=os.getenv('CEREBELUM_CORE_URL'))

    # TODO: Implementar approve_execution en SDK
    # await executor.approve_execution(
    #     execution_id=data.execution_id,
    #     approval_type="athlete_session_feedback",
    #     approval_data={
    #         'rating': data.rating,
    #         'objectives_met': data.objectives_met,
    #         'comments': data.comments
    #     }
    # )

    return {'message': 'Thank you for your feedback!'}
```

---

## ğŸ“Š Workflows Implementados

### 1. Athlete Onboarding (`01_athlete_onboarding.py`)

**Trigger:** POST `/api/auth/register`

**Flujo:**
1. Valida registro
2. Espera 7 dÃ­as que complete perfil (approval)
3. Habilita bookings

**DuraciÃ³n:** Hasta 7 dÃ­as

**Casos de uso:**
- Atleta completa perfil inmediatamente â†’ workflow termina en segundos
- Atleta tarda 3 dÃ­as â†’ workflow continÃºa despuÃ©s
- Atleta nunca completa â†’ timeout a los 7 dÃ­as

### 2. Booking Request (`02_booking_request.py`)

**Trigger:** POST `/api/bookings`

**Flujo:**
1. Valida slot disponible
2. Verifica balance
3. Crea booking
4. Reserva slot
5. Descuenta crÃ©dito
6. Confirma
7. Notifica coach y atleta (paralelo)

**DuraciÃ³n:** <1 segundo

**Ventajas:**
- AtÃ³mico (si algo falla, rollback completo)
- Notificaciones no bloquean confirmaciÃ³n

### 3. Session Completion (`03_session_completion.py`)

**Trigger:** POST `/api/bookings/{id}/complete`

**Flujo:**
1. Registra tiempo real trabajado
2. **PARALELO:**
   - Solicita feedback a atleta (timeout 48h)
   - Solicita evaluaciÃ³n a coach (timeout 7d)
3. Finaliza sesiÃ³n

**DuraciÃ³n:** Hasta 7 dÃ­as

**CaracterÃ­sticas:**
- Workflows pueden durar dÃ­as/semanas
- Timeouts automÃ¡ticos
- Evaluaciones en paralelo
- Sobrevive a reinicios del sistema

### 4. Payment Report (`04_payment_report.py`)

**Trigger:** POST `/api/admin/reports/payment` o Cron Job

**Flujo:**
1. Consulta sesiones completadas
2. Calcula total horas
3. Agrupa por semana
4. Genera reporte
5. Guarda en DB
6. Exporta a CSV
7. Notifica admin

**DuraciÃ³n:** ~5 segundos

**Uso:**
- Manual: Admin ejecuta desde UI
- AutomÃ¡tico: Cron job mensual

---

## ğŸ—„ï¸ Acceso a tu Base de Datos

### OpciÃ³n 1: Importar Repositorios Directamente

**Recomendado** - Reutiliza tu cÃ³digo existente

```python
# En tus workflow steps
from app.domain.repositories.booking_repository import BookingRepository
from app.domain.repositories.user_repository import UserRepository

booking_repo = BookingRepository()
user_repo = UserRepository()

async def create_booking_step(ctx, inputs):
    # Usar tu repository existente
    booking = await booking_repo.create({
        'athlete_id': inputs['athlete_id'],
        'coach_id': inputs['coach_id'],
        'scheduled_at': inputs['slot_datetime'],
        'status': 'PENDING'
    })

    return {'booking_id': booking.id}
```

### OpciÃ³n 2: Dependency Injection

Si usas DI en FastAPI:

```python
# app/workflows/dependencies.py
from app.domain.repositories import get_booking_repository

async def create_booking_step(ctx, inputs):
    # Obtener repository vÃ­a DI
    booking_repo = get_booking_repository()

    booking = await booking_repo.create(...)
    return {'booking_id': booking.id}
```

### OpciÃ³n 3: Shared Database Connection Pool

```python
# app/database.py
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

engine = create_async_engine(os.getenv('DATABASE_URL'))
AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession)

# En workflows
async def some_step(ctx, inputs):
    async with AsyncSessionLocal() as session:
        result = await session.execute("SELECT * FROM bookings WHERE id = :id", {'id': inputs['booking_id']})
        booking = result.fetchone()
        return {'booking': booking}
```

**IMPORTANTE:**
- âœ… Workers pueden acceder directamente a tu PostgreSQL
- âœ… Usan tu connection string existente
- âœ… Comparten el mismo pool de conexiones
- âœ… Respetan transacciones y locking

---

## ğŸ³ Docker Compose

### `docker-compose.yml`

```yaml
version: '3.8'

services:
  # Tu aplicaciÃ³n FastAPI existente
  fastapi-app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres:5432/zea_sport_db
      - CEREBELUM_CORE_URL=cerebelum-core:9090
    depends_on:
      - postgres
      - cerebelum-core
    volumes:
      - ./app:/app

  # Tu PostgreSQL existente
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: zea_sport_db
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # Cerebelum Core (NUEVO)
  cerebelum-core:
    image: cerebelum/core:latest  # TODO: publicar imagen
    ports:
      - "9090:9090"
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres-cerebelum:5432/cerebelum_db
      - ENABLE_WORKFLOW_RESURRECTION=true
    depends_on:
      - postgres-cerebelum

  # PostgreSQL para Cerebelum Event Store (NUEVO)
  postgres-cerebelum:
    image: postgres:15
    environment:
      POSTGRES_DB: cerebelum_db
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
    ports:
      - "5433:5432"  # Puerto diferente para no conflictuar
    volumes:
      - cerebelum_postgres_data:/var/lib/postgresql/data

  # Python Workers (NUEVO)
  cerebelum-worker:
    build: .
    command: python -m app.workers.main
    environment:
      - CEREBELUM_CORE_URL=cerebelum-core:9090
      - DATABASE_URL=postgresql://user:pass@postgres:5432/zea_sport_db
      - CEREBELUM_WORKER_ID=zea-sport-worker-${HOSTNAME:-1}
    depends_on:
      - cerebelum-core
      - postgres
    volumes:
      - ./app:/app
    deploy:
      replicas: 2  # MÃºltiples workers para paralelismo

volumes:
  postgres_data:
  cerebelum_postgres_data:
```

### `app/workers/main.py`

```python
#!/usr/bin/env python3
"""
Worker principal que ejecuta workflows de ZEA Sport.
"""

import asyncio
import os
from cerebelum import Worker

# Importar todos los workflows
from app.workflows.athlete_onboarding import build_athlete_onboarding_workflow
from app.workflows.booking_request import build_booking_request_workflow
from app.workflows.session_completion import build_session_completion_workflow
from app.workflows.payment_report import build_payment_report_workflow


async def main():
    core_url = os.getenv('CEREBELUM_CORE_URL', 'localhost:9090')
    worker_id = os.getenv('CEREBELUM_WORKER_ID', 'zea-sport-worker-1')

    print(f"Starting worker: {worker_id}")
    print(f"Connecting to Core: {core_url}")

    worker = Worker(core_url=core_url, worker_id=worker_id)

    try:
        await worker.run()
    except KeyboardInterrupt:
        print("Worker stopped by user")
    except Exception as e:
        print(f"Worker error: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ”§ Desarrollo Local

### 1. Levantar Cerebelum Core

```bash
# OpciÃ³n A: Docker Compose (recomendado)
docker-compose up cerebelum-core postgres-cerebelum

# OpciÃ³n B: Local (si tienes Elixir instalado)
cd /path/to/cerebelum-core
mix deps.get
mix ecto.create
mix ecto.migrate
mix run --no-halt
```

### 2. Levantar Worker

```bash
# Terminal separada
cd /path/to/your-fastapi-app
source venv/bin/activate
python -m app.workers.main
```

### 3. Levantar FastAPI

```bash
# Terminal separada
uvicorn app.main:app --reload --port 8000
```

### 4. Testear

```bash
# Registrar atleta
curl -X POST http://localhost:8000/api/auth/register \
  -H "Content-Type: application/json" \
  -d '{"email": "juan@email.com", "password": "secret123"}'

# Monitorear workflow
cd /path/to/cerebelum-core/examples/python-sdk
./cerebelum_cli.py list
./cerebelum_cli.py status <execution-id>
```

---

## ğŸ“Š Monitoreo y Debugging

### CLI de Cerebelum

```bash
# Instalar
cd /path/to/cerebelum-core/examples/python-sdk
pip install click
chmod +x cerebelum_cli.py

# Listar executions
./cerebelum_cli.py list

# Ver estado detallado
./cerebelum_cli.py status <execution-id>

# Monitorear en tiempo real
./cerebelum_cli.py watch <execution-id>

# Resumir workflow fallido
./cerebelum_cli.py resume <execution-id>

# Ver workflows activos
./cerebelum_cli.py active
```

### ExecutionClient (ProgramÃ¡tico)

```python
from cerebelum import ExecutionClient, ExecutionState

client = ExecutionClient(core_url="localhost:9090")

# Listar workflows activos
active = await client.list_active_workflows()
for exec in active:
    print(f"{exec.workflow_name}: {exec.progress_percentage}%")

# Ver estado detallado
status = await client.get_execution_status("exec-123")
print(f"Progress: {status.progress_percentage}%")
print(f"Current Step: {status.current_step_name}")

# Listar fallidos
failed, total, _ = await client.list_executions(
    status=ExecutionState.FAILED
)

client.close()
```

### Logging

```python
# En tus workflows, usa logging estÃ¡ndar
import logging

logger = logging.getLogger(__name__)

async def some_step(ctx, inputs):
    logger.info(f"Processing booking {inputs['booking_id']}")
    # ...
    logger.error(f"Failed to create booking: {error}")
```

---

## â“ Preguntas Frecuentes

### Â¿CÃ³mo integro con mi aplicaciÃ³n FastAPI existente?

1. Instalar `cerebelum-sdk`
2. Definir workflows en `app/workflows/`
3. Importar tus repositorios existentes en los steps
4. Ejecutar workflows desde tus endpoints usando `DistributedExecutor`
5. Levantar workers que ejecuten los steps

### Â¿CÃ³mo accedo a mi base de datos PostgreSQL?

Importa tus repositorios existentes directamente en los steps. Los workers tienen acceso completo a tu DB.

### Â¿Cerebelum reemplaza mi arquitectura?

NO. Cerebelum es un **complemento** para workflows asÃ­ncronos de larga duraciÃ³n. Tu arquitectura Clean Architecture con FastAPI sigue igual.

### Â¿Necesito cambiar mi schema de DB?

NO. Cerebelum usa su propia DB separada solo para event sourcing. Tu DB no se modifica.

### Â¿QuÃ© pasa si Cerebelum Core se cae?

Cuando vuelve a levantarse, **automÃ¡ticamente resucita workflows pausados** gracias a event sourcing. Los workflows continÃºan desde donde quedaron.

### Â¿CÃ³mo manejo workflows que esperan dÃ­as?

Usa `ctx.request_approval()` con `timeout_ms`. El workflow quedarÃ¡ en estado `WAITING_FOR_APPROVAL` y continuarÃ¡ automÃ¡ticamente cuando:
- Reciba aprobaciÃ³n (feedback del usuario)
- Timeout expire (auto-completa)

### Â¿Puedo tener mÃºltiples workers?

SÃ. Levanta mÃºltiples procesos de workers para paralelismo. Cerebelum distribuye automÃ¡ticamente los steps entre workers disponibles.

### Â¿CÃ³mo hago rollback si algo falla?

Si un step falla, el workflow completo falla y NO ejecuta steps siguientes. Implementa lÃ³gica de compensaciÃ³n si necesitas deshacer cambios ya hechos.

### Â¿Event sourcing es suficiente para analytics?

Event sourcing te da auditorÃ­a completa. Para analytics especÃ­ficos, consulta tus tablas normales de PostgreSQL (bookings, users, etc.).

---

## ğŸ“š Referencias

- [Cerebelum Documentation](../../docs/)
- [Python SDK Reference](../RESUMEN_PYTHON_SDK.md)
- [ExecutionClient API](../EXECUTION_CLIENT_README.md)
- [CLI Guide](../CLI_README.md)
- [Long-Running Workflows](../../docs/long-running-workflows.md)

---

## ğŸ¯ PrÃ³ximos Pasos

1. âœ… Revisar los 4 ejemplos de workflows
2. âœ… Copiar `docker-compose.yml` a tu proyecto
3. âœ… Crear `app/workflows/` con tus workflows
4. âœ… Crear `app/workers/main.py` con tu worker
5. âœ… Modificar endpoints para ejecutar workflows
6. âœ… Levantar todo con `docker-compose up`
7. âœ… Testear workflows con el CLI

Â¡Listo para empezar! ğŸš€
