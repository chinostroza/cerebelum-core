# Scalability Architecture - Competing with Temporal.io

**Status:** Design
**Created:** 2025-11-02
**Goal:** Achieve horizontal scalability to compete with Temporal.io while maintaining Elixir/OTP simplicity

---

## Table of Contents

1. [Competitive Analysis](#competitive-analysis)
2. [Scalability Goals](#scalability-goals)
3. [Multi-Tier Architecture](#multi-tier-architecture)
4. [Distributed Execution with Horde](#distributed-execution-with-horde)
5. [Partitioning Strategy](#partitioning-strategy)
6. [Caching Layer](#caching-layer)
7. [Event Storage at Scale](#event-storage-at-scale)
8. [Clustering & Service Discovery](#clustering--service-discovery)
9. [Performance Benchmarks](#performance-benchmarks)
10. [Deployment Topology](#deployment-topology)

---

## Competitive Analysis

### Temporal.io Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Temporal Cluster                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Frontend Service (gRPC)                                â”‚
â”‚      â†“                                                   â”‚
â”‚  History Service (Workflow state)                       â”‚
â”‚      â†“                                                   â”‚
â”‚  Matching Service (Task queues)                         â”‚
â”‚      â†“                                                   â”‚
â”‚  Worker Service (Executes tasks)                        â”‚
â”‚      â†“                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Cassandra/   â”‚  â”‚ Elasticsearchâ”‚  â”‚  Prometheus  â”‚ â”‚
â”‚  â”‚ PostgreSQL   â”‚  â”‚ (Visibility) â”‚  â”‚  (Metrics)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: 5+ services to operate
```

**Scalability**: Excelente, pero a costa de complejidad operacional.

### Cerebelum Architecture (Our Advantage)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Cerebelum Cluster                       â”‚
â”‚                  (Single Elixir Release)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  Phoenix HTTP/WebSocket (API)                           â”‚
â”‚      â†“                                                   â”‚
â”‚  Horde.Registry (Distributed lookup)                    â”‚
â”‚      â†“                                                   â”‚
â”‚  Horde.DynamicSupervisor (Distributed execution)        â”‚
â”‚      â†“                                                   â”‚
â”‚  ExecutionEngine (gen_statem) x 1,000,000               â”‚
â”‚      â†“                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ PostgreSQL   â”‚  â”‚ Phoenix.PubSubâ”‚                   â”‚
â”‚  â”‚ (Events)     â”‚  â”‚ (Real-time)   â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total: 1 service (Elixir) + 1 database
```

**Scalability**: Excelente + Simplicidad operacional.

---

## Scalability Goals

### Tier 1: Small Team (MVP)
- **Workflows**: 10,000 concurrent executions
- **Throughput**: 1,000 workflows/second
- **Nodes**: 1-3 nodes
- **Database**: Single PostgreSQL instance

### Tier 2: Growing Startup
- **Workflows**: 100,000 concurrent executions
- **Throughput**: 10,000 workflows/second
- **Nodes**: 3-10 nodes
- **Database**: PostgreSQL with read replicas

### Tier 3: Enterprise (Compete with Temporal)
- **Workflows**: 1,000,000+ concurrent executions
- **Throughput**: 100,000+ workflows/second
- **Nodes**: 10-100+ nodes
- **Database**: PostgreSQL with sharding or distributed DB (CockroachDB)

---

## Multi-Tier Architecture

### Tier 1: Single Node (Development/Small Teams)

```elixir
defmodule Cerebelum.Application do
  def start(_type, _args) do
    children = [
      # Database
      Cerebelum.Repo,

      # Local Registry (no clustering yet)
      {Registry, keys: :unique, name: Cerebelum.ExecutionRegistry},

      # Single supervisor
      Cerebelum.ExecutionSupervisor,

      # HTTP API
      CerebelumWeb.Endpoint
    ]

    Supervisor.start_link(children, strategy: :one_for_one)
  end
end
```

**Capacity**: ~10,000 concurrent workflows en una VM con 8 cores / 16GB RAM

### Tier 2: Clustered (3-10 nodes)

```elixir
defmodule Cerebelum.Application do
  def start(_type, _args) do
    children = [
      # Database with pooling
      Cerebelum.Repo,

      # Auto-discovery de nodos
      {Cluster.Supervisor, [topologies(), [name: Cerebelum.ClusterSupervisor]]},

      # Distributed Registry (Horde)
      {Horde.Registry,
        name: Cerebelum.DistributedRegistry,
        keys: :unique,
        members: :auto  # Auto-discovery de nodos
      },

      # Distributed Supervisor (Horde)
      {Horde.DynamicSupervisor,
        name: Cerebelum.DistributedSupervisor,
        strategy: :one_for_one,
        distribution_strategy: Horde.UniformQuorumDistribution,
        members: :auto
      },

      # Phoenix PubSub distribuido
      {Phoenix.PubSub, name: Cerebelum.PubSub, adapter: Phoenix.PubSub.PG2},

      # Partitioned cache (ETS)
      Cerebelum.Cache,

      # HTTP API
      CerebelumWeb.Endpoint
    ]

    Supervisor.start_link(children, strategy: :one_for_one)
  end

  defp topologies do
    [
      cerebelum: [
        strategy: Cluster.Strategy.Kubernetes.DNS,
        config: [
          service: "cerebelum-headless",
          application_name: "cerebelum"
        ]
      ]
    ]
  end
end
```

**Capacity**: ~100,000 concurrent workflows con 5 nodos x (8 cores / 16GB RAM)

### Tier 3: Massively Distributed (10-100+ nodes)

```elixir
defmodule Cerebelum.Application do
  def start(_type, _args) do
    children = [
      # Database connection pool (mÃ¡s grande)
      {Cerebelum.Repo, pool_size: 50},

      # Auto-discovery
      {Cluster.Supervisor, [topologies(), [name: Cerebelum.ClusterSupervisor]]},

      # Horde con quorum distribution
      {Horde.Registry, registry_config()},
      {Horde.DynamicSupervisor, supervisor_config()},

      # Phoenix Tracker para presence
      {Phoenix.Tracker, tracker_config()},

      # Partitioned supervisors (mÃ¡s particiones)
      {PartitionSupervisor,
        child_spec: Cerebelum.ExecutionWorkerPool,
        name: Cerebelum.PartitionedWorkers,
        partitions: System.schedulers_online() * 4
      },

      # Distributed cache (mÃºltiples ETS tables)
      Cerebelum.DistributedCache,

      # Metrics & observability
      Cerebelum.Telemetry,
      Cerebelum.Metrics.Exporter,

      # PubSub distribuido
      {Phoenix.PubSub, pubsub_config()},

      # HTTP API
      CerebelumWeb.Endpoint
    ]

    Supervisor.start_link(children, strategy: :one_for_one)
  end

  defp registry_config do
    [
      name: Cerebelum.DistributedRegistry,
      keys: :unique,
      members: :auto,
      # Delta-CRDT para eventually consistent registry
      delta_crdt_options: [sync_interval: 100]
    ]
  end

  defp supervisor_config do
    [
      name: Cerebelum.DistributedSupervisor,
      strategy: :one_for_one,
      # Quorum distribution - requiere mayorÃ­a de nodos
      distribution_strategy: Horde.UniformQuorumDistribution,
      members: :auto,
      # Process redistribution en 5 segundos
      process_redistribution: :active,
      delta_crdt_options: [sync_interval: 100]
    ]
  end

  defp pubsub_config do
    [
      name: Cerebelum.PubSub,
      adapter: Phoenix.PubSub.PG2,
      # Pool size para broadcast distribuido
      pool_size: System.schedulers_online() * 2
    ]
  end
end
```

**Capacity**: 1,000,000+ concurrent workflows con 50+ nodos

---

## Distributed Execution with Horde

### Why Horde?

Horde es **la clave** para competir con Temporal en escalabilidad:

```elixir
defmodule Cerebelum.ExecutionSupervisor do
  @moduledoc """
  Distributed execution supervisor using Horde.

  Key features:
  - Automatic failover: If a node dies, workflows restart on other nodes
  - Load balancing: Workflows distributed across all nodes
  - No single point of failure
  - Transparent to workflow code
  """

  alias Horde.DynamicSupervisor

  def start_execution(workflow_module, inputs, opts \\ []) do
    execution_id = generate_execution_id()

    child_spec = {
      Cerebelum.ExecutionEngine,
      [
        workflow_module: workflow_module,
        inputs: inputs,
        execution_id: execution_id,
        # Registry distribuido via Horde
        name: {:via, Horde.Registry,
               {Cerebelum.DistributedRegistry, execution_id}}
      ] ++ opts
    }

    # Horde decide en quÃ© nodo iniciar el proceso
    case DynamicSupervisor.start_child(Cerebelum.DistributedSupervisor, child_spec) do
      {:ok, pid} ->
        Logger.info("Started execution #{execution_id} on node #{node(pid)}")
        {:ok, %{id: execution_id, pid: pid, node: node(pid)}}

      {:error, {:already_started, pid}} ->
        {:ok, %{id: execution_id, pid: pid, node: node(pid)}}

      error ->
        error
    end
  end

  @doc """
  Lookup execution - works from any node in cluster
  """
  def lookup_execution(execution_id) do
    case Horde.Registry.lookup(Cerebelum.DistributedRegistry, execution_id) do
      [{pid, _value}] -> {:ok, pid}
      [] -> {:error, :not_found}
    end
  end

  @doc """
  Get all executions across the entire cluster
  """
  def list_all_executions do
    Horde.Registry.select(Cerebelum.DistributedRegistry, [{{:"$1", :"$2", :"$3"}, [], [{{:"$1", :"$2", :"$3"}}]}])
  end

  @doc """
  Get cluster statistics
  """
  def cluster_stats do
    members = Horde.DynamicSupervisor.members(Cerebelum.DistributedSupervisor)

    stats = Enum.map(members, fn member_node ->
      children = count_children_on_node(member_node)

      %{
        node: member_node,
        executions: children,
        load: get_node_load(member_node)
      }
    end)

    %{
      total_nodes: length(members),
      total_executions: Enum.sum(Enum.map(stats, & &1.executions)),
      nodes: stats
    }
  end

  defp count_children_on_node(node) do
    # Contar procesos en un nodo especÃ­fico
    Horde.Registry.select(Cerebelum.DistributedRegistry, [
      {{:"$1", :"$2", :"$3"}, [{:==, {:node, :"$2"}, node}], [true]}
    ])
    |> length()
  end

  defp get_node_load(node) do
    :rpc.call(node, :cpu_sup, :util, [])
  end

  defp generate_execution_id do
    UUID.uuid4()
  end
end
```

### Automatic Failover

```elixir
# Escenario: Nodo2 se cae
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Nodo1   â”‚  â”‚ Nodo2   â”‚  â”‚ Nodo3   â”‚
â”‚         â”‚  â”‚         â”‚  â”‚         â”‚
â”‚ Exec1   â”‚  â”‚ Exec2   â”‚  â”‚ Exec3   â”‚
â”‚ Exec4   â”‚  â”‚ Exec5   â”‚  â”‚ Exec6   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€ğŸ’¥â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
       Nodo2 crashes!
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Nodo1   â”‚               â”‚ Nodo3   â”‚
â”‚         â”‚               â”‚         â”‚
â”‚ Exec1   â”‚               â”‚ Exec3   â”‚
â”‚ Exec4   â”‚               â”‚ Exec6   â”‚
â”‚ Exec2   â”‚  â† Moved      â”‚ Exec5   â”‚  â† Moved
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Horde automÃ¡ticamente redistribuye Exec2 y Exec5
# Los workflows continÃºan desde el Ãºltimo evento guardado
```

---

## Partitioning Strategy

### Execution Partitioning

```elixir
defmodule Cerebelum.Partitioning do
  @moduledoc """
  Partition workflows across nodes and cores for maximum parallelism.

  Strategy:
  1. Hash execution_id â†’ determine partition
  2. Each partition = isolated supervisor + worker pool
  3. Partitions distributed across nodes via Horde
  """

  @partitions_per_node 16  # 2x CPU cores typically

  def partition_for_execution(execution_id) do
    total_partitions = @partitions_per_node * cluster_size()
    :erlang.phash2(execution_id, total_partitions)
  end

  def start_partitioned_execution(workflow_module, inputs) do
    execution_id = UUID.uuid4()
    partition = partition_for_execution(execution_id)

    # Iniciar en el worker pool de la particiÃ³n correspondiente
    child_spec = {
      Cerebelum.ExecutionEngine,
      [
        workflow_module: workflow_module,
        inputs: inputs,
        execution_id: execution_id,
        partition: partition
      ]
    }

    # Via PartitionSupervisor
    {:ok, pid} = DynamicSupervisor.start_child(
      {:via, PartitionSupervisor, {Cerebelum.PartitionedWorkers, partition}},
      child_spec
    )

    {:ok, %{id: execution_id, pid: pid, partition: partition}}
  end

  defp cluster_size do
    length(Node.list()) + 1  # +1 for current node
  end
end
```

### Visual Partitioning

```
Cluster de 3 nodos, 8 cores cada uno = 48 particiones totales

Node1 (partitions 0-15)      Node2 (partitions 16-31)    Node3 (partitions 32-47)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PartitionSup        â”‚      â”‚ PartitionSup        â”‚      â”‚ PartitionSup        â”‚
â”‚  â”œâ”€ P0  (Exec1,4,7) â”‚      â”‚  â”œâ”€ P16 (Exec2)     â”‚      â”‚  â”œâ”€ P32 (Exec3)     â”‚
â”‚  â”œâ”€ P1  (Exec10)    â”‚      â”‚  â”œâ”€ P17 (Exec5,8)   â”‚      â”‚  â”œâ”€ P33 (Exec6,9)   â”‚
â”‚  â”œâ”€ P2  (Exec11,14) â”‚      â”‚  â”œâ”€ P18 (...)       â”‚      â”‚  â”œâ”€ P34 (...)       â”‚
â”‚  â”œâ”€ ...             â”‚      â”‚  â”œâ”€ ...             â”‚      â”‚  â”œâ”€ ...             â”‚
â”‚  â””â”€ P15 (...)       â”‚      â”‚  â””â”€ P31 (...)       â”‚      â”‚  â””â”€ P47 (...)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
- Zero contention entre particiones
- Load balancing automÃ¡tico (hash distribution)
- Isolation: problemas en P0 no afectan P1
```

---

## Caching Layer

### Three-Level Cache

```elixir
defmodule Cerebelum.Cache do
  @moduledoc """
  Three-level caching strategy:

  L1: Persistent Term (fastest, immutable, shared globally)
  L2: ETS (fast, mutable, local to node)
  L3: Distributed cache (slower, shared across cluster)
  """

  # L1: Workflow metadata (immutable)
  def get_workflow_metadata(module) do
    case :persistent_term.get({:workflow_metadata, module}, nil) do
      nil ->
        metadata = extract_and_cache_metadata(module)
        metadata

      metadata ->
        metadata
    end
  end

  defp extract_and_cache_metadata(module) do
    metadata = Cerebelum.Workflow.Metadata.extract(module)
    :persistent_term.put({:workflow_metadata, module}, metadata)
    metadata
  end

  # L2: Execution lookup (ETS - local node)
  def init_ets do
    :ets.new(:execution_cache, [
      :named_table,
      :public,
      read_concurrency: true,
      write_concurrency: true
    ])
  end

  def cache_execution_state(execution_id, state) do
    :ets.insert(:execution_cache, {execution_id, state, :os.system_time(:second)})
  end

  def get_execution_state(execution_id) do
    case :ets.lookup(:execution_cache, execution_id) do
      [{^execution_id, state, _timestamp}] -> {:ok, state}
      [] -> :miss
    end
  end

  # L3: Distributed cache (via Nebulex)
  defmodule Distributed do
    use Nebulex.Cache,
      otp_app: :cerebelum,
      adapter: Nebulex.Adapters.Partitioned,
      primary_storage_adapter: Nebulex.Adapters.Local
  end

  def get_distributed(key) do
    Distributed.get(key)
  end

  def put_distributed(key, value, ttl \\ :timer.hours(1)) do
    Distributed.put(key, value, ttl: ttl)
  end
end
```

### Cache Performance

```
Operation             | Persistent Term | ETS        | Distributed
---------------------|-----------------|------------|-------------
Read (local)         | ~10 ns          | ~100 ns    | ~1 ms
Write                | âŒ Immutable    | ~100 ns    | ~2 ms
Shared across nodes  | No              | No         | Yes
Use case             | Metadata        | Hot data   | Session data
```

---

## Event Storage at Scale

### Partitioned Event Store

```elixir
defmodule Cerebelum.EventStore.Partitioned do
  @moduledoc """
  Partition events across multiple tables for write scalability.

  Strategy:
  - Hash execution_id â†’ partition number
  - Each partition = separate table
  - Queries can be parallelized across partitions
  """

  @num_partitions 64

  def append(execution_id, event, version) do
    partition = partition_for(execution_id)
    table = "events_#{partition}"

    query = """
    INSERT INTO #{table} (execution_id, event_type, event_data, version, inserted_at)
    VALUES ($1, $2, $3, $4, $5)
    """

    Repo.query(query, [
      execution_id,
      event_type(event),
      Jason.encode!(event),
      version,
      DateTime.utc_now()
    ])
  end

  def get_events(execution_id) do
    partition = partition_for(execution_id)
    table = "events_#{partition}"

    query = """
    SELECT event_type, event_data, version, inserted_at
    FROM #{table}
    WHERE execution_id = $1
    ORDER BY version ASC
    """

    Repo.query!(query, [execution_id])
    |> decode_events()
  end

  defp partition_for(execution_id) do
    :erlang.phash2(execution_id, @num_partitions)
  end
end
```

### Migration for Partitioned Tables

```elixir
defmodule Cerebelum.Repo.Migrations.CreatePartitionedEvents do
  use Ecto.Migration

  def up do
    # Create 64 event tables
    for partition <- 0..63 do
      create table("events_#{partition}", primary_key: false) do
        add :id, :uuid, primary_key: true
        add :execution_id, :string, null: false
        add :event_type, :string, null: false
        add :event_data, :jsonb, null: false
        add :version, :integer, null: false
        add :inserted_at, :utc_datetime_usec, null: false
      end

      create index("events_#{partition}", [:execution_id, :version])
      create unique_index("events_#{partition}", [:execution_id, :version])
    end
  end

  def down do
    for partition <- 0..63 do
      drop table("events_#{partition}")
    end
  end
end
```

### Write Performance

```
Single table:
- 10,000 writes/sec (limited by single table lock)

64 partitions:
- 640,000 writes/sec (64x parallelism)
```

---

## Clustering & Service Discovery

### libcluster Configuration

```elixir
# config/runtime.exs
config :libcluster,
  topologies: [
    k8s: [
      strategy: Cluster.Strategy.Kubernetes.DNS,
      config: [
        service: "cerebelum-headless",
        application_name: "cerebelum",
        polling_interval: 5_000
      ]
    ],

    # Fallback to Gossip for non-K8s
    gossip: [
      strategy: Cluster.Strategy.Gossip,
      config: [
        port: 45892,
        multicast_addr: "230.1.1.251",
        multicast_ttl: 1,
        secret: System.get_env("CLUSTER_SECRET")
      ]
    ]
  ]
```

### Kubernetes Deployment

```yaml
# k8s/cerebelum-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cerebelum
spec:
  serviceName: cerebelum-headless
  replicas: 10
  selector:
    matchLabels:
      app: cerebelum
  template:
    metadata:
      labels:
        app: cerebelum
    spec:
      containers:
      - name: cerebelum
        image: cerebelum:latest
        ports:
        - containerPort: 4000
          name: http
        - containerPort: 4369
          name: epmd
        - containerPort: 9100
          name: distributed
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: RELEASE_DISTRIBUTION
          value: "name"
        - name: RELEASE_NODE
          value: "cerebelum@$(POD_IP)"
        resources:
          requests:
            memory: "2Gi"
            cpu: "2000m"
          limits:
            memory: "4Gi"
            cpu: "4000m"
---
apiVersion: v1
kind: Service
metadata:
  name: cerebelum-headless
spec:
  clusterIP: None
  selector:
    app: cerebelum
  ports:
  - port: 4369
    name: epmd
```

---

## Performance Benchmarks

### Target: Match Temporal.io

| Metric | Temporal.io | Cerebelum (Target) | Strategy |
|--------|-------------|-------------------|----------|
| **Concurrent workflows** | 1M+ | 1M+ | Horde + Partitioning |
| **Throughput** | 100K/sec | 100K/sec | Partitioned event store |
| **Latency (p99)** | <100ms | <50ms | BEAM advantage |
| **Node failure recovery** | <5s | <2s | Horde failover |
| **Database load** | High | Medium | ETS caching |
| **Memory per workflow** | ~1KB | ~500 bytes | Lightweight processes |
| **Operational complexity** | 5+ services | 1 service | OTP advantage |

### Estimated Capacity per Node

```elixir
# Node specs: 8 cores, 16GB RAM
# BEAM theoretical max: ~134 million processes
# Practical limit with state: ~100,000 processes per node

defmodule Cerebelum.Capacity do
  # Conservative estimate
  @workflows_per_core 12_500
  @cores_per_node 8
  @workflows_per_node @workflows_per_core * @cores_per_node  # 100,000

  def max_capacity(num_nodes) do
    %{
      nodes: num_nodes,
      workflows_per_node: @workflows_per_node,
      total_capacity: @workflows_per_node * num_nodes,
      recommended_max: trunc(@workflows_per_node * num_nodes * 0.8)  # 80% capacity
    }
  end
end

# Ejemplos:
Capacity.max_capacity(10)
# => %{
#   nodes: 10,
#   workflows_per_node: 100_000,
#   total_capacity: 1_000_000,
#   recommended_max: 800_000  # Leave 20% headroom
# }

Capacity.max_capacity(50)
# => %{
#   nodes: 50,
#   workflows_per_node: 100_000,
#   total_capacity: 5_000_000,
#   recommended_max: 4_000_000
# }
```

---

## Deployment Topology

### Small Deployment (Tier 1)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Load Balancer               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚ Node1  â”‚â—„â”€â”€â”€â”€â–ºâ”‚ Node2   â”‚
â”‚ (8core)â”‚      â”‚ (8core) â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
      â”‚ PostgreSQL  â”‚
      â”‚ (Primary)   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Capacity: ~200K workflows
Cost: ~$500/month
```

### Medium Deployment (Tier 2)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Load Balancer (HA)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚         â”‚         â”‚
â”Œâ”€â”€â–¼â”€â”€â”   â”Œâ”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”
â”‚Node1â”‚â—„â”€â–ºâ”‚Node2â”‚â—„â–ºâ”‚Node3â”‚
â”‚     â”‚   â”‚     â”‚  â”‚     â”‚
â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜
   â”‚        â”‚         â”‚
â”Œâ”€â”€â–¼â”€â”€â”   â”Œâ”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”
â”‚Node4â”‚â—„â”€â–ºâ”‚Node5â”‚â—„â–ºâ”‚Node6â”‚
â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”˜
   â”‚        â”‚         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
     â”‚ PostgreSQL  â”‚
     â”‚ (Primary)   â”‚
     â”‚      +      â”‚
     â”‚  Replicas   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Capacity: ~600K workflows
Cost: ~$2,000/month
```

### Large Deployment (Tier 3 - Enterprise)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Global Load Balancerâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                â”‚                â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
         â”‚ Region 1 â”‚    â”‚ Region 2 â”‚    â”‚ Region 3 â”‚
         â”‚ 10 nodes â”‚    â”‚ 10 nodes â”‚    â”‚ 10 nodes â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚                â”‚                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Distributed Database (CockroachDB)           â”‚
    â”‚         (Multi-region, auto-sharding)                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Capacity: 3M+ workflows
Cost: ~$20,000/month
Uptime: 99.99%
```

---

## Implementation Phases

### Phase 1: Foundation (Weeks 1-4)
- âœ… Registry + ETS cache
- âœ… Task.Supervisor for parallel
- âœ… Basic telemetry

### Phase 2: Clustering (Weeks 5-8)
- ğŸ”² Horde.Registry
- ğŸ”² Horde.DynamicSupervisor
- ğŸ”² libcluster integration
- ğŸ”² Phoenix.PubSub distributed

### Phase 3: Partitioning (Weeks 9-12)
- ğŸ”² PartitionSupervisor
- ğŸ”² Partitioned event store
- ğŸ”² Distributed cache (Nebulex)

### Phase 4: Enterprise (Weeks 13-16)
- ğŸ”² Multi-region support
- ğŸ”² Advanced monitoring
- ğŸ”² Auto-scaling
- ğŸ”² Performance testing at scale

---

## Competitive Advantages Summary

### vs Temporal.io

| Aspect | Temporal.io | Cerebelum |
|--------|-------------|-----------|
| **Setup complexity** | 5+ services | 1 service |
| **Memory per workflow** | ~1KB | ~500 bytes |
| **Latency** | ~100ms (gRPC overhead) | ~20ms (native BEAM) |
| **Hot reload** | âŒ Restart required | âœ… Hot code reload |
| **Observability** | External tools | Built-in (Observer, Trace) |
| **Language** | Go (workers in any lang) | Elixir (native concurrency) |
| **Cost** | High (many services) | Low (single service) |

### vs LangGraph

| Aspect | LangGraph | Cerebelum |
|--------|-----------|-----------|
| **Concurrency** | Limited (Python GIL) | Unlimited (BEAM) |
| **Scalability** | Vertical | Horizontal |
| **Use case** | AI/ML workflows | General + AI workflows |
| **Durability** | Limited | Full event sourcing |
| **Clustering** | âŒ | âœ… Native |

---

## Next Steps

1. **Update Implementation Plan**
   - Add Horde to Phase 2
   - Add Partitioning to Phase 3
   - Add benchmarking tasks

2. **Create Performance Tests**
   - Benchmark 100K workflows on single node
   - Benchmark 1M workflows on 10 nodes
   - Measure failover time

3. **Documentation**
   - Deployment guide for K8s
   - Scaling guide
   - Monitoring guide

4. **Marketing**
   - "Simpler than Temporal, More Scalable than LangGraph"
   - Benchmark comparisons
   - Case studies

---

**The Cerebelum Advantage**: Enterprise-grade scalability with startup simplicity, powered by battle-tested OTP.
